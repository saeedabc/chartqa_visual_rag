{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbee46b0-e235-4edb-a471-bbff177e2f27",
   "metadata": {},
   "source": [
    "## Benchmark RAG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f3e076-2b31-4e18-931d-12cecde8058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import *\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "best_refined_path = f\"{TEAM_ROOT_DIR}/refined_chartqa/val-100_qwen25_vl_32b_instruct\"\n",
    "\n",
    "def run_isolated(ret_model_name: str = \"colpali13\", \n",
    "                 gen_model_name: str = \"qwen25_vl_7b_instruct\", \n",
    "                 dset_path: str = best_refined_path,\n",
    "                 force_update: bool = False):\n",
    "\n",
    "    command = [\n",
    "        sys.executable, \"-m\", \"multi_modal\", \n",
    "        \"--ret_model_name\", ret_model_name,\n",
    "        \"--gen_model_name\", gen_model_name,\n",
    "        \"--dset_path\", dset_path,\n",
    "    ]\n",
    "    if force_update:\n",
    "        command.append(\"--force_update\")\n",
    "        \n",
    "    subprocess.run(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5829fac0-d351-46db-8221-e0a49d4a7543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images already stored at /projects/multimodal_bootcamp/multimodal-td-2/shared/chartqa_images/val\n",
      "Verbosity is set to 1 (active). Pass verbose=0 to make quieter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 20815.40it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.66s/it]\n",
      "/fs01/home/ws_sabbasi/projects/chartqa_visual_rag/.myvenv/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:25<00:00,  5.10s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "Visual RAG Inference and Evaluation: 100%|██████████| 86/86 [05:04<00:00,  3.55s/it]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 86/86 [00:00<00:00, 1642.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving RAG results to /projects/multimodal_bootcamp/multimodal-td-2/shared/refined_chartqa/val-100_qwen25_vl_32b_instruct_rag_results_86_colpali13_qwen25_vl_7b_instruct\n"
     ]
    }
   ],
   "source": [
    "run_isolated(ret_model_name=\"colpali13\", gen_model_name=\"qwen25_vl_7b_instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d27b7593-ba5f-4fa4-94b7-d86e03daf76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images already stored at /projects/multimodal_bootcamp/multimodal-td-2/shared/chartqa_images/val\n",
      "Verbosity is set to 1 (active). Pass verbose=0 to make quieter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 33156.55it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.15s/it]\n",
      "/fs01/home/ws_sabbasi/projects/chartqa_visual_rag/.myvenv/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|██████████| 18/18 [00:48<00:00,  2.71s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "Visual RAG Inference and Evaluation: 100%|██████████| 86/86 [21:27<00:00, 14.97s/it] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 86/86 [00:00<00:00, 2143.66 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parsing error: Invalid \\escape: line 2 column 1324 (char 1325)\n",
      "Raw string: {\n",
      "  \"answer\": \"To determine the total sum of the highest values (peak points) recorded for all three categories ('Mostly good news,' 'Mostly bad news,' and 'A mix of good and bad news') across the entire timeline, we need to analyze each chart individually and identify the peak value for each category. Then, we will sum these peak values.\\n\\n### Chart Analysis:\\n1. **Chart 1 (May 12-15, 2011):**\\n   - **Mostly good news:** The highest value is 7.\\n   - **Mostly bad news:** The highest value is 68.\\n   - **A mix of good and bad news:** The highest value is 68.\\n\\n2. **Chart 2 (September 1-4, 2011):**\\n   - **Mostly good news:** The highest value is 5.\\n   - **Mostly bad news:** The highest value is 67.\\n   - **A mix of good and bad news:** The highest value is 65.\\n\\n3. **Chart 3 (August 4-7, 2011):**\\n   - **Mostly good news:** The highest value is 7.\\n   - **Mostly bad news:** The highest value is 67.\\n   - **A mix of good and bad news:** The highest value is 68.\\n\\n### Peak Values Across All Charts:\\n- **Mostly good news:** The highest peak is 7 (from both Chart 1 and Chart 3).\\n- **Mostly bad news:** The highest peak is 68 (from Chart 1 and Chart 3).\\n- **A mix of good and bad news:** The highest peak is 68 (from Chart 1 and Chart 3).\\n\\n### Sum of Peak Values:\\nThe total sum of the highest values is \\( 7 + 68 + 68 = 143 \\).\",\n",
      "  \"short_answer\": \"143\",\n",
      "  \"references\": [1, 2, 3]\n",
      "}\n",
      "JSON parsing error: Invalid control character at: line 2 column 187 (char 188)\n",
      "Raw string: {\n",
      "  \"answer\": \"The question asks whether the pie chart displays data categorized into three distinct segments. Upon examining the provided charts, we need to analyze each one individually:\n",
      "\n",
      "1. **Chart 1**: This pie chart shows multiple segments representing different categories such as 'Children's', 'Shoes', 'Accessories, lingerie, fine jewelry', 'Men’s', 'Ladies', and 'Home accents'. There are more than three segments here, so this chart does not meet the criteria of having only three distinct segments.\n",
      "\n",
      "2. **Chart 2**: This pie chart has two segments labeled 'Domestics merchandise' and 'Home furnishings'. Since there are only two segments, this chart also does not meet the criteria of having three distinct segments.\n",
      "\n",
      "3. **Chart 3**: This pie chart displays several segments representing income ranges, such as 'Under 15,000', '15,000 to 24,999', '25,000 to 34,999', etc. Similar to Chart 1, there are more than three segments, so this chart does not meet the criteria either.\n",
      "\n",
      "None of the provided charts display data categorized into exactly three distinct segments. Therefore, based on the given charts, the answer is no.\",\n",
      "  \"short_answer\": \"No\",\n",
      "  \"references\": [1, 2, 3]\n",
      "}\n",
      "Saving RAG results to /projects/multimodal_bootcamp/multimodal-td-2/shared/refined_chartqa/val-100_qwen25_vl_32b_instruct_rag_results_86_colpali13_qwen25_vl_32b_instruct\n"
     ]
    }
   ],
   "source": [
    "run_isolated(ret_model_name=\"colpali13\", gen_model_name=\"qwen25_vl_32b_instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a84846c-33f0-4371-a60a-0a31b4dffec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images already stored at /projects/multimodal_bootcamp/multimodal-td-2/shared/chartqa_images/val\n",
      "Verbosity is set to 1 (active). Pass verbose=0 to make quieter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 19195.90it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.70s/it]\n",
      "/fs01/home/ws_sabbasi/projects/chartqa_visual_rag/.myvenv/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:25<00:00,  5.09s/it]\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "Visual RAG Inference and Evaluation: 100%|██████████| 86/86 [04:50<00:00,  3.38s/it]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 86/86 [00:00<00:00, 2345.44 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving RAG results to /projects/multimodal_bootcamp/multimodal-td-2/shared/refined_chartqa/val-100_qwen25_vl_32b_instruct_rag_results_86_colqwen2010_qwen25_vl_7b_instruct\n"
     ]
    }
   ],
   "source": [
    "run_isolated(ret_model_name=\"colqwen2010\", gen_model_name=\"qwen25_vl_7b_instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "007bd924-3087-4446-883b-7714ae71d4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images already stored at /projects/multimodal_bootcamp/multimodal-td-2/shared/chartqa_images/val\n",
      "Verbosity is set to 1 (active). Pass verbose=0 to make quieter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 13819.78it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 13.44s/it]\n",
      "/fs01/home/ws_sabbasi/projects/chartqa_visual_rag/.myvenv/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|██████████| 18/18 [00:54<00:00,  3.04s/it]\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "Visual RAG Inference and Evaluation: 100%|██████████| 86/86 [20:51<00:00, 14.56s/it] \n",
      "Saving the dataset (0/1 shards):   0%|          | 0/86 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parsing error: Invalid control character at: line 2 column 326 (char 327)\n",
      "Raw string: {\n",
      "  \"answer\": \"To determine the total sum of the highest values (peak points) recorded for all three categories ('Mostly good news,' 'Mostly bad news,' and 'A mix of good and bad news') across the entire timeline, we need to identify the peak value for each category from the provided charts. Here's the step-by-step reasoning:\n",
      "\n",
      "1. **Identify the peak values for each category:**\n",
      "   - **'Mostly good news':** This category consistently remains low throughout the timeline, with the highest value being 7 in January 2011.\n",
      "   - **'Mostly bad news':** The highest value for this category is 67, which occurs in August 2011.\n",
      "   - **'A mix of good and bad news':** The highest value for this category is 68, which occurs in January 2011.\n",
      "\n",
      "2. **Sum the peak values:**\n",
      "   - Peak for 'Mostly good news': 7\n",
      "   - Peak for 'Mostly bad news': 67\n",
      "   - Peak for 'A mix of good and bad news': 68\n",
      "\n",
      "   Total sum = 7 + 67 + 68 = 142\n",
      "\n",
      "3. **Verification:** All three charts show consistent data for these categories, so the identified peak values are accurate.\n",
      "\n",
      "Therefore, the total sum of the highest values recorded for all three categories is 142.\"\n",
      "\n",
      "  \"short_answer\": \"142\",\n",
      "  \"references\": [1, 2, 3]\n",
      "}\n",
      "Saving RAG results to /projects/multimodal_bootcamp/multimodal-td-2/shared/refined_chartqa/val-100_qwen25_vl_32b_instruct_rag_results_86_colqwen2010_qwen25_vl_32b_instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 86/86 [00:00<00:00, 1323.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "run_isolated(ret_model_name=\"colqwen2010\", gen_model_name=\"qwen25_vl_32b_instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fac3e32-fa41-49c7-89e1-b74e3b1c6857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined QAs path: /projects/multimodal_bootcamp/multimodal-td-2/shared/refined_chartqa/val-100_qwen25_vl_32b_instruct\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ret_precision</th>\n",
       "      <th>ret_recall</th>\n",
       "      <th>gen_correctness</th>\n",
       "      <th>gen_exact_match</th>\n",
       "      <th>gen_soft_match</th>\n",
       "      <th>gen_soft_match_partial</th>\n",
       "      <th>gen_precision</th>\n",
       "      <th>gen_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RAG (colpali13, qwen25_vl_7b_instruct):</th>\n",
       "      <td>31.78</td>\n",
       "      <td>95.35</td>\n",
       "      <td>69.77</td>\n",
       "      <td>44.19</td>\n",
       "      <td>69.90</td>\n",
       "      <td>76.97</td>\n",
       "      <td>91.47</td>\n",
       "      <td>95.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAG (colpali13, qwen25_vl_32b_instruct):</th>\n",
       "      <td>31.78</td>\n",
       "      <td>95.35</td>\n",
       "      <td>80.23</td>\n",
       "      <td>51.16</td>\n",
       "      <td>74.57</td>\n",
       "      <td>81.06</td>\n",
       "      <td>90.31</td>\n",
       "      <td>91.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAG (colqwen2010, qwen25_vl_7b_instruct):</th>\n",
       "      <td>31.40</td>\n",
       "      <td>94.19</td>\n",
       "      <td>62.79</td>\n",
       "      <td>43.02</td>\n",
       "      <td>66.58</td>\n",
       "      <td>73.12</td>\n",
       "      <td>91.09</td>\n",
       "      <td>94.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAG (colqwen2010, qwen25_vl_32b_instruct):</th>\n",
       "      <td>31.40</td>\n",
       "      <td>94.19</td>\n",
       "      <td>80.23</td>\n",
       "      <td>48.84</td>\n",
       "      <td>73.62</td>\n",
       "      <td>80.09</td>\n",
       "      <td>89.34</td>\n",
       "      <td>90.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            ret_precision  ret_recall  \\\n",
       "RAG (colpali13, qwen25_vl_7b_instruct):             31.78       95.35   \n",
       "RAG (colpali13, qwen25_vl_32b_instruct):            31.78       95.35   \n",
       "RAG (colqwen2010, qwen25_vl_7b_instruct):           31.40       94.19   \n",
       "RAG (colqwen2010, qwen25_vl_32b_instruct):          31.40       94.19   \n",
       "\n",
       "                                            gen_correctness  gen_exact_match  \\\n",
       "RAG (colpali13, qwen25_vl_7b_instruct):               69.77            44.19   \n",
       "RAG (colpali13, qwen25_vl_32b_instruct):              80.23            51.16   \n",
       "RAG (colqwen2010, qwen25_vl_7b_instruct):             62.79            43.02   \n",
       "RAG (colqwen2010, qwen25_vl_32b_instruct):            80.23            48.84   \n",
       "\n",
       "                                            gen_soft_match  \\\n",
       "RAG (colpali13, qwen25_vl_7b_instruct):              69.90   \n",
       "RAG (colpali13, qwen25_vl_32b_instruct):             74.57   \n",
       "RAG (colqwen2010, qwen25_vl_7b_instruct):            66.58   \n",
       "RAG (colqwen2010, qwen25_vl_32b_instruct):           73.62   \n",
       "\n",
       "                                            gen_soft_match_partial  \\\n",
       "RAG (colpali13, qwen25_vl_7b_instruct):                      76.97   \n",
       "RAG (colpali13, qwen25_vl_32b_instruct):                     81.06   \n",
       "RAG (colqwen2010, qwen25_vl_7b_instruct):                    73.12   \n",
       "RAG (colqwen2010, qwen25_vl_32b_instruct):                   80.09   \n",
       "\n",
       "                                            gen_precision  gen_recall  \n",
       "RAG (colpali13, qwen25_vl_7b_instruct):             91.47       95.35  \n",
       "RAG (colpali13, qwen25_vl_32b_instruct):            90.31       91.86  \n",
       "RAG (colqwen2010, qwen25_vl_7b_instruct):           91.09       94.19  \n",
       "RAG (colqwen2010, qwen25_vl_32b_instruct):          89.34       90.70  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from multi_modal import MultiModalRAG\n",
    "\n",
    "rag_metrics = {}\n",
    "for ret_model_name in [\"colpali13\", \"colqwen2010\"]:\n",
    "    for gen_model_name in [\"qwen25_vl_7b_instruct\", \"qwen25_vl_32b_instruct\"]:\n",
    "        results_path = f\"{TEAM_ROOT_DIR}/refined_chartqa/val-100_qwen25_vl_32b_instruct_rag_results_86_{ret_model_name}_{gen_model_name}\"\n",
    "        \n",
    "        results_dset = Dataset.load_from_disk(results_path)\n",
    "        metrics = MultiModalRAG.aggregate_metrics(results_dset)\n",
    "    \n",
    "        run_title = f\"RAG ({ret_model_name}, {gen_model_name}):\"\n",
    "        rag_metrics[run_title] = metrics\n",
    "\n",
    "print(f\"Refined QAs path: {best_refined_path}\")\n",
    "metrics_df = pd.DataFrame(rag_metrics).transpose().multiply(100).round(2)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf3fc0-bc54-4d32-a432-88322e77968d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
